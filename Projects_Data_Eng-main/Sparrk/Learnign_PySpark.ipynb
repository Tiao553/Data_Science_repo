{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "normal-tolerance",
   "metadata": {},
   "source": [
    "<center><img src=\"http://spark.apache.org/images/spark-logo.png\" height=100></center>\n",
    "\n",
    "\n",
    "> ## Neste curso vamos aprender os fundamentos sobre o uso do Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-airfare",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Na prática, **o cluster será hospedado em uma máquina remota conectada a todos os outros nós.** Haverá um computador, chamado mestre, que gerencia a divisão dos dados e dos cálculos. O mestre é conectado aos demais computadores do cluster, que são chamados de trabalhador . O mestre envia os dados e cálculos dos trabalhadores para execução, e eles enviam seus resultados de volta ao mestre.\n",
    "\n",
    "Quando você está começando a usar o Spark, é mais simples apenas executar um cluster localmente.\n",
    "\n",
    "Criar a conexão é tão simples quanto criar uma instância da `SparkContext` classe. O construtor de classe leva alguns argumentos opcionais que permitem especificar os atributos do cluster ao qual você está se conectando. Com esta função nos conectamos ao cluster.\n",
    "\n",
    "Um objeto contendo todos esses atributos pode ser criado com o `SparkConf()` construtor. Dê uma olhada na [documentação](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession) para todos os detalhes!\n",
    "Para o restante deste curso, você terá uma `SparkContext` chamada `sc` já disponível em sua área de trabalho.\n",
    "\n",
    "**Spark tem sob seu capô são projetadas para operações complicadas com conjuntos de big data. Isso significa que, para problemas simples ou pequenos, o Spark pode ter um desempenho pior do que algumas outras soluções!**\n",
    "--\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-event",
   "metadata": {},
   "source": [
    "---\n",
    "# Usando DataFrames\n",
    "\n",
    "A estrutura de dados central do Spark é o Resilient Distributed Dataset (RDD). Este é um objeto de baixo nível que permite ao Spark trabalhar sua mágica, dividindo dados em vários nós no cluster. No entanto, é difícil trabalhar com RDDs diretamente, portanto, neste curso, você usará a abstração Spark DataFrame construída sobre RDDs.\n",
    "\n",
    "O Spark DataFrame foi projetado para se comportar muito como uma tabela SQL (uma tabela com variáveis nas colunas e observações nas linhas). Além de serem mais fáceis de entender, os DataFrames também são mais otimizados para operações complicadas do que os RDDs.\n",
    "\n",
    "Quando você começa a modificar e combinar colunas e linhas de dados, há muitas maneiras de chegar ao mesmo resultado, mas algumas geralmente demoram muito mais do que outras. Ao usar RDDs, cabe ao cientista de dados descobrir a maneira certa de otimizar a consulta, mas a implementação do DataFrame tem muito dessa otimização embutida!\n",
    "\n",
    "Para começar a trabalhar com o Spark DataFrames, primeiro você deve criar um `SparkSession` objeto a partir do seu `SparkContext`. Você pode pensar em `SparkContext` como sua conexão com o cluster e `SparkSession` como sua interface com essa conexão.\n",
    "\n",
    "Lembre-se, no restante deste curso, você terá uma SparkSession chamada spark disponível em sua área de trabalho!\n",
    "\n",
    "Já criamos um SparkSession para você chamado spark, mas e se você não tiver certeza de que já existe um? Criar vários `SparkSession` e `SparkContext` pode causar problemas, portanto, é uma prática recomendada usar o `SparkSession.builder.getOrCreate()` método. Isso retorna um existente, `SparkSession` se já houver um no ambiente, ou cria um novo, se necessário!\n",
    "--\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-reverse",
   "metadata": {},
   "source": [
    "---\n",
    "# Vendo as tabelas presentes\n",
    "\n",
    "Depois de criar um `SparkSession`, você pode começar a vasculhar para ver quais dados estão em seu cluster!\n",
    "\n",
    "Seu `SparkSession` tem um atributo chamado `catalog` que lista todos os dados dentro do cluster. Este atributo possui alguns métodos para extrair diferentes informações.\n",
    "--\n",
    "Um dos mais úteis é o `.listTables()` método, que retorna os nomes de todas as tabelas em seu cluster como uma lista.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-intention",
   "metadata": {},
   "source": [
    "---\n",
    "# Relacionando com SQL\n",
    "\n",
    "Executar uma consulta nesta tabela é tão fácil quanto usar o `.sql()` método no seu `SparkSession`. Este método pega uma string contendo a consulta e retorna um DataFrame com os resultados!\n",
    "--\n",
    "Isso ocorre porque não há um objeto local em seu ambiente que contenha esses dados, então não faria sentido passar a tabela como um argumento.\n",
    "\n",
    "Criaremos um `SparkSession` chamado `spark` em seu espaço de trabalho.\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/sql.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-strengthening",
   "metadata": {},
   "source": [
    "---\n",
    "# Pandafy a Spark DataFrame\n",
    "\n",
    "Suponha que você tenha executado uma consulta em seu enorme conjunto de dados e o agregado em algo um pouco mais gerenciável.\n",
    "\n",
    "Às vezes, faz sentido pegar essa table e trabalhar com ela localmente usando uma ferramenta como `pandas`. O Spark DataFrames facilita isso com o `.toPandas()` método. Chamar esse método em um Spark DataFrame retorna o `pandasDataFrame` correspondente . É simples assim!\n",
    "--\n",
    "\n",
    "Lembre-se, já existe um `SparkSession` chamado `spark` em seu espaço de trabalho!\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/pandafy.png\" height=\"100%\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-argument",
   "metadata": {},
   "source": [
    "---\n",
    "# Coloque um pouco de Spark em seus dados\n",
    "\n",
    "Talvez você queira ir na outra direção e colocar um pandasDataFrame em um cluster Spark! A `SparkSession` classe também tem um método para isso.O .`createDataFrame()` método usa um pandasDataFrame e retorna um Spark DataFrame.\n",
    "--\n",
    "\n",
    "A saída desse método é armazenada localmente, não no `SparkSession` catálogo. Isso significa que você pode usar todos os métodos Spark DataFrame nele, mas não pode acessar os dados em outros contextos.\n",
    "\n",
    "Por exemplo, uma consulta SQL (usando o `.sql()` método) que faz referência ao seu DataFrame gerará um erro. Para acessar os dados desta forma, você deve salvá-los como uma tabela temporária .\n",
    "\n",
    "Você pode fazer isso usando o `.createTempView()` método Spark DataFrame, que tem como único argumento o nome da tabela temporária que você gostaria de registrar. Este método registra o DataFrame como uma tabela no catálogo, mas como esta tabela é temporária, ela só pode ser acessada a partir do específico `SparkSession` usado para criar o Spark DataFrame.\n",
    "\n",
    "Também existe o método `.createOrReplaceTempView()`. Isso cria com segurança uma nova tabela temporária, se não houver nada lá antes, ou atualiza uma tabela existente, se já houver uma definida. Você usará esse método para evitar problemas com tabelas duplicadas.\n",
    "\n",
    "Confira o diagrama para ver todas as diferentes maneiras como as estruturas de dados do Spark interagem umas com as outras.\n",
    "--\n",
    "<center><img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/course_4452/datasets/spark_figure.png\" height=\"100%\"></center>\n",
    "\n",
    "Já existe um `SparkSession` chamado `spark` em seu espaço de trabalho, `numpy` foi importado como `np` e `pandas` como `pd`.\n",
    "--\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-crash",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-argentina",
   "metadata": {},
   "source": [
    "---\n",
    "# Abandonando o intermediário\n",
    "\n",
    "Agora você sabe como colocar dados no Spark via `pandas`, mas provavelmente está se perguntando por que lidar com `pandas` isso?\n",
    "\n",
    "Não seria mais fácil simplesmente ler um arquivo de texto direto no Spark? \n",
    "--\n",
    "# Claro que sim!\n",
    "\n",
    "Felizmente, seu `SparkSession` tem um `.read` atributo que possui vários métodos para ler diferentes fontes de dados em Spark DataFrames. Usando isso, você pode criar um DataFrame a partir de um arquivo .csv, assim como pandasacontece com DataFrames normais !\n",
    "\n",
    "A variável file_pathé uma string com o caminho para o arquivo airports.csv. Este arquivo contém informações sobre diferentes aeroportos em todo o mundo.\n",
    "Um SparkSessionnome sparkestá disponível em sua área de trabalho.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/sapr_in_data.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-theorem",
   "metadata": {},
   "source": [
    "---\n",
    "# Criando colunas\n",
    "\n",
    "Neste capítulo, você aprenderá como usar os métodos definidos pela `DataFrame` classe do Spark para realizar operações de dados comuns.\n",
    "\n",
    "Vejamos como realizar operações em colunas. No Spark, você pode fazer isso usando o `.withColumn()` método, que leva dois argumentos. Primeiro, uma string com o nome de sua nova coluna e, em seguida, a própria nova coluna.\n",
    "--\n",
    "A nova coluna deve ser um objeto de classe `Column`. Criar um deles é tão fácil quanto extrair uma coluna de seu DataFrame usando `df.colName`.\n",
    "\n",
    "Atualizar um Spark DataFrame é um pouco diferente de trabalhar `pandas` porque o Spark DataFrame é imutável . Isso significa que ele não pode ser alterado e, portanto, as colunas não podem ser atualizadas no local.\n",
    "\n",
    "Portanto, todos esses métodos retornam um novo DataFrame. Para substituir o DataFrame original, você deve reatribuir o DataFrame retornado usando o método assim:\n",
    "\n",
    "`df = df.withColumn(\"newCol\", df.oldCol + 1)`\n",
    "--\n",
    "\n",
    "O código acima cria um DataFrame com as mesmas colunas dfmais uma nova coluna ,, newColonde cada entrada é igual à entrada correspondente de `oldCol`, mais um.\n",
    "\n",
    "Para sobrescrever uma coluna existente, basta passar o nome da coluna como primeiro argumento!\n",
    "\n",
    "Lembre-se de que uma `SparkSession` chamada `spark`já está em sua área de trabalho\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/sql1.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-tours",
   "metadata": {},
   "source": [
    "---\n",
    "# SQL em poucas palavras\n",
    "\n",
    "**Tem que estudar o conceito como um todo mas vamos a alguns pontos para norteamento**\n",
    "---\n",
    "Conforme você avança, será útil ter um conhecimento básico de SQL. Uma análise mais aprofundada pode ser encontrada aqui .\n",
    "\n",
    "Uma consulta SQL retorna uma tabela derivada de uma ou mais tabelas contidas em um banco de dados.\n",
    "\n",
    "Cada consulta SQL é composta de comandos que informam ao banco de dados o que você deseja fazer com os dados. Os dois comandos que cada consulta deve conter são SELECTe FROM.\n",
    "\n",
    "* `SELECT` comando é seguido pelas colunas que você deseja na tabela resultante.\n",
    "\n",
    "* `FROM` comando é seguido pelo nome da tabela que contém essas colunas. A consulta SQL mínima é:\n",
    "\n",
    "SELECT * FROM my_table;\n",
    "* `*`seleciona todas as colunas, portanto, retorna a tabela inteira nomeada my_table.\n",
    "\n",
    "Semelhante a .withColumn(), você pode fazer cálculos em colunas dentro de uma SELECTinstrução. Por exemplo,\n",
    "\n",
    "`SELECT origin, dest, air_time / 60 FROM flights;`\n",
    "--\n",
    "retorna uma tabela com a origem, o destino e a duração em horas de cada voo.\n",
    "\n",
    "Outro comando comumente usado é WHERE. Este comando filtra as linhas da tabela com base em alguma condição lógica que você especificar. A tabela resultante contém as linhas em que sua condição é verdadeira. Por exemplo, se você tivesse uma tabela de alunos e notas, poderia fazer:\n",
    "\n",
    "`SELECT * FROM students WHERE grade = 'A';`\n",
    "--\n",
    "para selecionar todas as colunas e linhas que contêm informações sobre os alunos que obtiveram As."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-selling",
   "metadata": {},
   "source": [
    "Outra tarefa comum de banco de dados é a agregação. Ou seja, reduzindo seus dados dividindo-os em partes e resumindo cada parte.\n",
    "--\n",
    "\n",
    "Isso é feito em `SQL` usando o `GROUP BY` comando. Este comando divide seus dados em grupos e aplica uma função de sua `SELECT` declaração a cada grupo.\n",
    "\n",
    "Por exemplo, se você quiser contar o número de voos de cada um dos dois destinos de origem, poderá usar a consulta\n",
    "\n",
    "`SELECT COUNT(*) FROM flights GROUP BY origin;`\n",
    "--\n",
    "\n",
    "`GROUP BY` origininforma ao `SQL` que você deseja que a saída tenha uma linha para cada valor exclusivo da origincoluna. A `SELECT` instrução seleciona os valores que você deseja preencher cada uma das colunas. Aqui, queremos `COUNT()` cada linha em cada um dos grupos.\n",
    "\n",
    "É possível `GROUP BY` mais de uma coluna. Ao fazer isso, a tabela resultante terá uma linha para cada combinação dos valores exclusivos em cada coluna. A consulta a seguir conta o número de voos de SEA e PDX para cada aeroporto de destino:\n",
    "\n",
    "`SELECT origin, dest, COUNT(*) FROM flights GROUP BY origin, dest;`\n",
    "--\n",
    "\n",
    "A saída terá uma linha para cada combinação dos valores em origine dest(ou seja, uma linha listando cada origem e destino para onde um vôo voou). Haverá também uma coluna com o número `COUNT()` de todas as linhas em cada grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-margin",
   "metadata": {},
   "source": [
    "---\n",
    "# Filtrando Dados\n",
    "\n",
    "Agora que você tem um pouco de know-how em SQL, é mais fácil falar sobre as operações análogas usando Spark DataFrames.\n",
    "--\n",
    "\n",
    "Vamos dar uma olhada no `.filter()` método. Como você pode suspeitar, esta é a contraparte Spark da `WHERE` cláusula `SQL` . O .`filter()` método usa uma expressão que seguiria a `WHERE` cláusula de uma expressão `SQL` como uma string ou uma coluna Spark de valores booleanos `( True/ False)`.\n",
    "\n",
    "Por exemplo, as duas expressões a seguir produzirão a mesma saída:\n",
    "--\n",
    "\n",
    "`flights.filter(\"air_time > 120\").show()`\n",
    "--\n",
    "`flights.filter(flights.air_time > 120).show()`\n",
    "--\n",
    "\n",
    "Observe que, no primeiro caso, passamos uma string para `.filter()`. Em `SQL`, escreveríamos essa tarefa de filtragem como `SELECT * FROM flights WHERE air_time > 120`. O Spark `.filter()` pode aceitar qualquer expressão que possa entrar na `WHERE` cláusula de uma consulta SQL (neste caso, \"`air_time > 120`\"), desde que seja passada como string. Observe que, neste caso, não fazemos referência ao nome da tabela na string - como não faríamos na solicitação SQL.\n",
    "\n",
    "No segundo caso, realmente passamos uma coluna de valores booleanos para `.filter()`. Lembre-se de que `flights.air_time > 120` retorna uma coluna de valores booleanos que tem `True` no lugar desses registros `flights.air_time` que são superiores a 120, e `False` caso contrário.\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/filter.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-concept",
   "metadata": {},
   "source": [
    "---\n",
    "# Selecionando\n",
    "\n",
    "A variante do Spark do SQL `SELECT` é o .`select()` método. Este método aceita vários argumentos - um para cada coluna que você deseja selecionar. Esses argumentos podem ser o nome da coluna como uma string (um para cada coluna) ou um objeto de coluna (usando a `df.colName` sintaxe). Quando você passa um objeto de coluna, você pode realizar operações como adição ou subtração na coluna para alterar os dados contidos nela, bem como dentro `.withColumn()`.\n",
    "\n",
    "A diferença entre os métodos `.select()` e `.withColumn()` é que `.select()` retorna apenas as colunas que você especificar, enquanto `.withColumn()` retorna todas as colunas do DataFrame além daquela que você definiu. Muitas vezes, é uma boa ideia soltar colunas desnecessárias no início de uma operação, para não arrastar dados extras ao fazer o wrangling. Nesse caso, você usaria `.select()` e não `.withColumn()`.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/select.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-colonial",
   "metadata": {},
   "source": [
    "Semelhante ao SQL, você também pode usar o `.select()` método para realizar operações em colunas.\n",
    "--\n",
    "\n",
    "Ao selecionar uma coluna usando a `df.colName` notação, você pode realizar qualquer operação de coluna e o `.select()`método retornará a coluna transformada. Por exemplo:\n",
    "\n",
    "`flights.select(flights.air_time/60)`\n",
    "--\n",
    "\n",
    "retorna uma coluna de durações de voos em horas em vez de minutos. Você também pode usar o `.alias()` método para renomear uma coluna que está selecionando. Então, se você quiser que `.select()` a coluna `duration_hrs`(que não está em seu DataFrame), você pode fazer:\n",
    "\n",
    "`flights.select((flights.air_time/60).alias(\"duration_hrs\"))`\n",
    "--\n",
    "\n",
    "O método Spark DataFrame equivalente `.selectExpr()` usa expressões SQL como uma string:\n",
    "\n",
    "`flights.selectExpr(\"air_time/60 as duration_hrs\")`\n",
    "--\n",
    "\n",
    "com a `as` palavra-chave SQL sendo equivalente ao `.alias()` método. Para selecionar várias colunas, você pode passar várias strings.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/select2.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-prague",
   "metadata": {},
   "source": [
    "---\n",
    "# Agregador\n",
    "\n",
    "Todos os métodos de agregação comum, como `.min()`, `.max()`, e `.count()` são `GroupedData` métodos. Eles são criados chamando o `.groupBy()` método DataFrame. Você aprenderá exatamente o que isso significa em alguns exercícios. Por enquanto, tudo que você precisa fazer para usar essas funções é chamar esse método em seu DataFrame. Por exemplo, para encontrar o valor mínimo de uma coluna `col`, em um DataFrame, `df` você poderia fazer:\n",
    "\n",
    "`df.groupBy().min(\"col\").show()`\n",
    "--\n",
    "\n",
    "Isso cria um `GroupedData` objeto (para que você possa usar o `.min()` método), localiza o valor mínimo em cole o retorna como um DataFrame.\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/aggregating.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-palmer",
   "metadata": {},
   "source": [
    "Outra forma de agregação é agregar os dados com alguma metrica estatistica seja `.count()` que é ua contagem de ocorreêcias, ou mesmo `.avg()` calcula a média das ocrreêcias, ou mesmo `.sum()` que realização da soma das ocorrências.\n",
    "--\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/aggregating2.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-freight",
   "metadata": {},
   "source": [
    "---\n",
    "# Agrupando e agregando I \n",
    "\n",
    "Parte do que torna a agregação tão poderosa é a adição de grupos. PySpark tem uma classe inteira dedicada a frames de dados agrupados: \n",
    "\n",
    "`pyspark.sql.GroupedData` que você viu nos últimos dois exercícios.\n",
    "\n",
    "Você aprendeu como criar um DataFrame agrupado chamando o `.groupBy()` método em um DataFrame sem argumentos.\n",
    "\n",
    "Agora você verá que quando você passa o nome de uma ou mais colunas em seu DataFrame para o `.groupBy()` método, os métodos de agregação se comportam como quando você usa uma `GROUP BY` instrução em uma consulta SQL!\n",
    "--\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/Agg_group1.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-unemployment",
   "metadata": {},
   "source": [
    "# Agrupamento e agregação II\n",
    "\n",
    "Além dos GroupedDatamétodos que você já viu, existe também o `.agg()` método. Este método permite passar uma expressão de coluna agregada que usa qualquer uma das funções agregadas do `pyspark.sql.functions` submódulo.\n",
    "\n",
    "Este submódulo contém muitas funções úteis para computar coisas como desvios-padrão. Todas as funções de agregação neste submódulo recebem o nome de uma coluna em uma `GroupedData` tabela.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/Agg_group2.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-compound",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-antigua",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "modern-wallpaper",
   "metadata": {},
   "source": [
    "---\n",
    "# Joining\n",
    "\n",
    "Outra operação de dados muito comum é a junção . As junções são um tópico inteiro em si, portanto, neste curso, veremos apenas junções simples. Se quiser saber mais sobre junções, você pode dar uma olhada aqui.\n",
    "--\n",
    "\n",
    "Uma junção combinará duas tabelas diferentes ao longo de uma coluna que elas compartilham. Esta coluna é chamada de chave . Exemplos de chaves aqui incluem as colunas `tailnum` e `carrier` da `flights` tabela.\n",
    "\n",
    "Por exemplo, suponha que você queira saber mais informações sobre o avião que fez um vôo do que apenas o número da cauda. Essas informações não estão na `flights` tabela porque o mesmo avião voa em muitos voos diferentes ao longo de dois anos, portanto, incluir essas informações em todas as linhas resultaria em muitas duplicações. Para evitar isso, você teria uma segunda tabela com apenas uma linha para cada plano e cujas colunas listam todas as informações sobre o plano, incluindo o número da cauda. Você poderia chamar esta mesa `planes`.\n",
    "\n",
    "Quando você associa a `flights` tabela a esta tabela de informações do avião, está adicionando todas as colunas da `planes` tabela à `flights` tabela. Para preencher essas colunas com informações, você examinará o número final da `flights` tabela e encontrará o número correspondente na `planes` tabela e, em seguida, usará essa linha para preencher todas as novas colunas.\n",
    "\n",
    "No `PySpark`, as junções são realizadas usando o método DataFrame `.join()`. Este método leva três argumentos. O primeiro é o segundo DataFrame que você deseja unir ao primeiro. O segundo argumento, `on` é o nome da(s) coluna(s) chave como uma string. Os nomes das colunas-chave devem ser os mesmos em cada tabela. O terceiro argumento, `how` especifica o tipo de junção a ser executada. Neste curso, sempre usaremos o valor `how=\"leftouter\"`.\n",
    "--\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/Tiao553/Projects_databases/main/Sparrk/img/joins.png\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-minister",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-optimum",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Construindo Machine Learning Direto na Pipeline\n",
    "\n",
    "Nos próximos dois capítulos, você percorrerá todos os estágios do pipeline de aprendizado de máquina, desde a entrada de dados até a avaliação do modelo. Vamos lá!\n",
    "--\n",
    "\n",
    "No centro do `pyspark.m` lmódulo estão as classes `Transformer` e `Estimator`. Quase todas as outras classes do módulo se comportam de maneira semelhante a essas duas classes básicas.\n",
    "\n",
    "`Transformer` as classes têm um `.transform()` método que pega um DataFrame e retorna um novo DataFrame; geralmente o original com uma nova coluna anexada. Por exemplo, você pode usar a classe `Bucketizer` para criar bins discretos a partir de um recurso contínuo ou a classe `PCA` para reduzir a dimensionalidade do seu conjunto de dados usando a análise do componente principal.\n",
    "\n",
    "`Estimator` todas as classes implementam um `.fit()` método. Esses métodos também usam um DataFrame, mas em vez de retornar outro DataFrame, eles retornam um objeto de modelo. Pode ser algo como um `StringIndexerModel` para incluir dados categóricos salvos como strings em seus modelos ou um `RandomForestModel` que usa o algoritmo de floresta aleatório para classificação ou regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-mississippi",
   "metadata": {},
   "source": [
    "---\n",
    "# Tipos de dados\n",
    "\n",
    "Bom trabalho! Antes de começar a modelar, é importante saber que o Spark trata apenas de dados numéricos. Isso significa que todas as colunas em seu DataFrame devem ser inteiros ou decimais (chamados de 'duplos' no Spark).\n",
    "--\n",
    "\n",
    "Quando importamos nossos dados, deixamos o Spark adivinhar que tipo de informação cada coluna continha. Infelizmente, o Spark nem sempre adivinha certo e você pode ver que algumas das colunas em nosso DataFrame são strings contendo números em oposição aos valores numéricos reais.\n",
    "\n",
    "Para remediar isso, você pode usar o `.cast()` método em combinação com o `.withColumn()` método. É importante notar que `.cast()` funciona em colunas, enquanto `.withColumn()` funciona em DataFrames.\n",
    "\n",
    "O único argumento que você precisa passar `.cast()` é o tipo de valor que você deseja criar, na forma de string. Por exemplo, para criar inteiros, você passará o argumento \"integer\"e para números decimais você usará \"double\".\n",
    "\n",
    "Você pode colocar esta chamada para `.cast()` dentro de uma chamada para `.withColumn()` para sobrescrever a coluna já existente.\n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/select2.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-buyer",
   "metadata": {},
   "source": [
    "---\n",
    "# Strings e factors\n",
    "\n",
    "Como você sabe, o Spark requer dados numéricos para modelagem. Até agora, isso não foi um problema; até mesmo colunas booleanas podem ser facilmente convertidas em inteiros sem nenhum problema. Mas você também usará a companhia aérea e o destino do avião como recursos em seu modelo. Eles são codificados como strings e não há uma maneira óbvia de convertê-los em um tipo de dados numérico.\n",
    "\n",
    "Felizmente, o PySpark tem funções para lidar com isso embutidas no `pyspark.ml.features` submódulo. Você pode criar os chamados 'vetores one-hot' para representar a transportadora e o destino de cada voo. Um vetor one-hot é uma forma de representar uma característica categórica em que cada observação tem um vetor em que todos os elementos são zero, exceto para no máximo um elemento, que tem o valor um (1).\n",
    "\n",
    "Cada elemento do vetor corresponde a um nível do recurso, então é possível dizer qual é o nível certo vendo qual elemento do vetor é igual a um (1).\n",
    "\n",
    "A primeira etapa para codificar seu recurso categórico é criar um `StringIndexer`. Os membros dessa classe são `Estimator`s que pegam um DataFrame com uma coluna de strings e mapeiam cada string exclusiva para um número. Em seguida, o `Estimator` retorna um `Transformer` que leva um DataFrame, anexa o mapeamento a ele como metadados e retorna um novo DataFrame com uma coluna numérica correspondente à coluna da string.\n",
    "\n",
    "A segunda etapa é codificar essa coluna numérica como um vetor one-hot usando a `OneHotEncoder`. Isso funciona exatamente da mesma maneira que `StringIndexer` criando um `Estimator` e, em seguida, um `Transformer`. O resultado final é uma coluna que codifica seu recurso categórico como um vetor adequado para rotinas de aprendizado de máquina!\n",
    "\n",
    "Isso pode parecer complicado, mas não se preocupe! Tudo que você precisa lembrar é que você precisa criar um `StringIndexer`e um `OneHotEncoder`, e o `Pipeline` cuidará do resto.\n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/Sindex%26OneHotencolder.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-backup",
   "metadata": {},
   "source": [
    "---\n",
    "# Assemble a vector\n",
    "\n",
    "A última etapa do `Pipeline` é combinar todas as colunas que contêm nossos recursos em uma única coluna. Isso deve ser feito antes que a modelagem possa ocorrer, porque cada rotina de modelagem do Spark espera que os dados estejam neste formato. Você pode fazer isso armazenando cada um dos valores de uma coluna como uma entrada em um vetor. Então, do ponto de vista do modelo, cada observação é um vetor que contém todas as informações sobre ela e um rótulo que diz ao modelador a qual valor essa observação corresponde.\n",
    "\n",
    "Por causa disso, o `pyspark.ml.feature` submódulo contém uma classe chamada `VectorAssembler`. Isso `Transformer` pega todas as colunas que você especifica e as combina em uma nova coluna de vetor.\n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/vectorass.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-apple",
   "metadata": {},
   "source": [
    "---\n",
    "# Crie o pipeline\n",
    "\n",
    "Você finalmente está pronto para criar um `Pipeline`!\n",
    "\n",
    "`Pipeline` é uma classe no `pyspark.ml` módulo que combina todos os `Estimators`e os `Transformers` que você já criou. Isso permite reutilizar o mesmo processo de modelagem repetidamente, envolvendo-o em um objeto simples. Legal, certo?\n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/createPipe.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-communication",
   "metadata": {},
   "source": [
    "---\n",
    "# Test vs Train\n",
    "\n",
    "Depois de limpar seus dados e deixá-los prontos para modelagem, uma das etapas mais importantes é dividir os dados em um conjunto de teste e um conjunto de treinamento . Depois disso, não toque em seus dados de teste até achar que tem um bom modelo! Ao construir modelos e formar hipóteses, você pode testá-los em seus dados de treinamento para ter uma ideia de seu desempenho.\n",
    "\n",
    "Depois de obter seu modelo favorito, você pode ver como ele prevê os novos dados em seu conjunto de teste. Esses dados nunca antes vistos darão uma ideia muito mais realista do desempenho do seu modelo no mundo real quando você estiver tentando prever ou classificar novos dados.\n",
    "\n",
    "No Spark, é importante certificar-se de dividir os dados após todas as transformações. Isso ocorre porque operações como `StringIndexer` nem sempre produzem o mesmo índice, mesmo quando dada a mesma lista de strings.\n",
    "\n",
    "Transforme os dados\n",
    "-- \n",
    "\n",
    "Viva, agora você está finalmente pronto para passar seus dados através do que Pipelinevocê criou!\n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/fittransform.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>\n",
    "\n",
    "Dividindo os dados\n",
    "--\n",
    "\n",
    "Agora que você fez todas as suas manipulações, a última etapa antes da modelagem é dividir os dados!\n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/split.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-anthropology",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-helping",
   "metadata": {},
   "source": [
    "---\n",
    "# Ajuste e seleção de modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-speaker",
   "metadata": {},
   "source": [
    "Cross validation\n",
    "--\n",
    "\n",
    "Utilizando o  modelo de regressão logística usando um procedimento chamado validação cruzada k-fold . Este é um método de estimar o desempenho do modelo em dados não vistos (como seu `test`DataFrame).\n",
    "\n",
    "Ele funciona dividindo os dados de treinamento em algumas partições diferentes. O número exato depende de você, mas neste curso você usará o valor padrão de três do PySpark. Depois que os dados são divididos, uma das partições é posta de lado e o modelo se ajusta às outras. Em seguida, o erro é medido em relação à partição mantida. Isso é repetido para cada uma das partições, de modo que cada bloco de dados seja mantido e usado como um conjunto de teste exatamente uma vez. Em seguida, é calculada a média do erro em cada uma das partições. Isso é chamado de erro de validação cruzada do modelo e é uma boa estimativa do erro real nos dados mantidos.\n",
    "\n",
    "Você usará a validação cruzada para escolher os hiperparâmetros criando uma grade dos pares de valores possíveis para os dois hiperparâmetros `elasticNetParam`e `regParam`, e usará o erro de validação cruzada para comparar todos os diferentes modelos para que possa escolher o melhor!\n",
    "\n",
    "Para isso precisamos seguir alguns passos até iniciarmos a cross validation, como \n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/evaluation.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>\n",
    "\n",
    "Tunning\n",
    "--\n",
    "Em seguida, você precisa criar uma grade de valores para pesquisar ao procurar os hiperparâmetros ideais. O submódulo pyspark.ml.tuning inclui uma classe chamada ParamGridBuilder que faz exatamente isso (talvez você esteja começando a notar um padrão aqui; o PySpark tem um submódulo para quase tudo!). \n",
    "\n",
    "Você precisará usar os métodos `.addGrid()` e `.build()` para criar uma grade que pode ser usada para cross validation. O método `.addGrid()` pega um parâmetro de modelo (um atributo do Estimator de modelo, `lr`, que você criou alguns exercícios atrás) e uma lista de valores que você deseja experimentar. O método `.build()` não aceita argumentos, apenas retorna a grade que você usará mais tarde.\n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/hipertunning.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>\n",
    "\n",
    "Fazendo a cross validation\n",
    "--\n",
    "\n",
    "O submódulo `pyspark.ml.tuning` também tem uma classe chamada `CrossValidator`para realizar a validação cruzada. Isso `Estimator` leva o modelador que você deseja ajustar, a grade de hiperparâmetros que você criou e o avaliador que deseja usar para comparar seus modelos.\n",
    "\n",
    "O submódulo `pyspark.ml.tune` já foi importado como `tune`. Você criará o `CrossValidator` passando a regressão logística `Estimator` `lr`, o parâmetro `grid` e o que `evaluator` você criou nos exercícios anteriores.\n",
    "\n",
    "<center><img src=\"http://localhost:8888/files/Desktop/data%20Scienci/dataengineer/datacamp_dataengineering/spark_datacamp/img/realizando.png?_xsrf=2%7C00e49229%7C20bc5ec3080bef52d7ffa82bdd3bf29e%7C1618366929\" height=\"100%\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
